"""
ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸: crawl_sites.json â†’ Supabase DB
"""
import json
import asyncio
from pathlib import Path
from supabase_client import supabase
from datetime import time


def load_json_data():
    """Load data from crawl_sites.json"""
    json_path = Path(__file__).parent / "crawl_sites.json"
    with open(json_path, 'r', encoding='utf-8') as f:
        return json.load(f)


async def migrate_data():
    """Migrate data from JSON to Supabase"""
    print("ğŸš€ Starting migration from crawl_sites.json to Supabase...")

    # 1. JSON ë°ì´í„° ë¡œë“œ
    data = load_json_data()
    sites = data.get("sites", [])

    print(f"ğŸ“‹ Found {len(sites)} sites in crawl_sites.json")

    # 2. ê¸°ë³¸ í´ë” ìƒì„± (ë§¤ì¼ ìƒˆë²½ 2ì‹œ)
    print("\nğŸ“ Creating default folder...")
    folder_data = {
        "name": "ê¸°ë³¸ í´ë”",
        "schedule_type": "daily",
        "schedule_time": "02:00:00",
        "schedule_day": None,
        "enabled": True
    }

    # ê¸°ì¡´ í´ë” í™•ì¸
    existing_folder = supabase.table("crawl_folders").select("*").eq("name", "ê¸°ë³¸ í´ë”").execute()

    if existing_folder.data:
        folder_id = existing_folder.data[0]["id"]
        print(f"âœ… Default folder already exists (ID: {folder_id})")
    else:
        result = supabase.table("crawl_folders").insert(folder_data).execute()
        folder_id = result.data[0]["id"]
        print(f"âœ… Created default folder (ID: {folder_id})")

    # 3. ì‚¬ì´íŠ¸ ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜
    print(f"\nğŸ“Š Migrating {len(sites)} sites...")

    # ê¸°ì¡´ ë°ì´í„° í™•ì¸
    existing_sites = supabase.table("scheduled_crawl_sites").select("url").eq("folder_id", folder_id).execute()
    existing_urls = {site["url"] for site in existing_sites.data}

    # ìƒˆë¡œ ì¶”ê°€í•  ì‚¬ì´íŠ¸ì™€ ì—…ë°ì´íŠ¸í•  ì‚¬ì´íŠ¸ ë¶„ë¦¬
    new_sites = []
    update_sites = []

    for site in sites:
        site_data = {
            "folder_id": folder_id,
            "name": site["name"],
            "url": site["url"],
            "description": site.get("description", ""),
            "enabled": site.get("enabled", False)
        }

        if site["url"] in existing_urls:
            update_sites.append(site_data)
        else:
            new_sites.append(site_data)

    # ìƒˆ ì‚¬ì´íŠ¸ ì‚½ì…
    if new_sites:
        # ë°°ì¹˜ë¡œ ì‚½ì… (í•œ ë²ˆì— 50ê°œì”©)
        batch_size = 50
        inserted_count = 0
        for i in range(0, len(new_sites), batch_size):
            batch = new_sites[i:i + batch_size]
            try:
                supabase.table("scheduled_crawl_sites").insert(batch).execute()
                inserted_count += len(batch)
            except Exception as e:
                print(f"âš ï¸  Error inserting batch {i//batch_size + 1}: {e}")
                # ê°œë³„ ì‚½ì… ì‹œë„
                for site in batch:
                    try:
                        supabase.table("scheduled_crawl_sites").insert([site]).execute()
                        inserted_count += 1
                    except Exception as e2:
                        print(f"   âŒ Failed to insert {site['name']}: {e2}")

        print(f"âœ… Inserted {inserted_count} new sites")
    else:
        print("â„¹ï¸  No new sites to insert")

    # ê¸°ì¡´ ì‚¬ì´íŠ¸ ì—…ë°ì´íŠ¸
    if update_sites:
        updated_count = 0
        for site in update_sites:
            try:
                supabase.table("scheduled_crawl_sites").update({
                    "enabled": site["enabled"],
                    "name": site["name"],
                    "description": site["description"]
                }).eq("folder_id", folder_id).eq("url", site["url"]).execute()
                updated_count += 1
            except Exception as e:
                print(f"âš ï¸  Failed to update {site['name']}: {e}")

        print(f"âœ… Updated {updated_count} existing sites")

    # 4. ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ í™•ì¸
    print("\nğŸ“ˆ Migration Summary:")
    folders = supabase.table("crawl_folders").select("*").execute()
    print(f"   - Total folders: {len(folders.data)}")

    all_sites = supabase.table("scheduled_crawl_sites").select("*").execute()
    enabled_count = len([s for s in all_sites.data if s["enabled"]])
    print(f"   - Total sites: {len(all_sites.data)}")
    print(f"   - Enabled sites: {enabled_count}")
    print(f"   - Disabled sites: {len(all_sites.data) - enabled_count}")

    print("\nâœ… Migration completed successfully!")


if __name__ == "__main__":
    asyncio.run(migrate_data())
