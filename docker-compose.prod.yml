services:
  vpn:
    image: qmcgaw/gluetun:latest
    container_name: rag-vpn
    cap_add:
      - NET_ADMIN
    devices:
      - /dev/net/tun:/dev/net/tun
    environment:
      - VPN_SERVICE_PROVIDER=custom
      - VPN_TYPE=wireguard
      - VPN_ENDPOINT_IP=185.107.56.156
      - VPN_ENDPOINT_PORT=51820
      - WIREGUARD_PUBLIC_KEY=hc5/jolTT72oU7vbiFruJ20LtWU2I/T19E3/15ax6yw=
      - WIREGUARD_PRIVATE_KEY=GBb6vcUDEhatbEJ8PiH/8hbEfnf+NGa3RM4KQEWyR30=
      - WIREGUARD_ADDRESSES=10.2.0.2/32
      - DNS_ADDRESS=10.2.0.1
      - TZ=Asia/Seoul
      - HTTPPROXY=on
      - SHADOWSOCKS=off
    ports:
      - "8888:8888"  # HTTP proxy port
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "sh", "-c", "test -f /tmp/gluetun/ip"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 30s

  nginx:
    image: nginx:alpine
    container_name: rag-nginx
    ports:
      - "9090:80"
      - "9443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - ./ssl:/etc/nginx/ssl:ro
      - ./certbot/www:/var/www/certbot:ro
    environment:
      - DOMAIN_NAME=${DOMAIN_NAME}
      - SSL_CERT_PATH=${SSL_CERT_PATH:-/etc/nginx/ssl/fullchain.pem}
      - SSL_KEY_PATH=${SSL_KEY_PATH:-/etc/nginx/ssl/privkey.pem}
    env_file:
      - .env
    depends_on:
      - frontend
      - vpn
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://127.0.0.1/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  api:
    build:
      context: ./backend
      args:
        PYTHON_VERSION: ${PYTHON_VERSION:-3.11}
    container_name: rag-api
    expose:
      - "8000"
    volumes:
      - ./backend:/app:ro
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - TZ=Asia/Seoul
      - VPN_PROXY_URL=http://rag-vpn:8888
    env_file:
      - .env
    depends_on:
      vpn:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_started
      ollama:
        condition: service_started
    command: bash -lc "source activate appenv && python main.py"
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    restart: unless-stopped

  celery:
    build:
      context: ./backend
      args:
        PYTHON_VERSION: ${PYTHON_VERSION:-3.11}
    container_name: rag-celery
    hostname: celery-crawler-worker
    volumes:
      - ./backend:/app:ro
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - TZ=Asia/Seoul
      - VPN_PROXY_URL=http://rag-vpn:8888
    env_file:
      - .env
    depends_on:
      vpn:
        condition: service_healthy
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_started
      ollama:
        condition: service_started
    command:
      - bash
      - -c
      - >
        source /opt/conda/bin/activate appenv &&
        celery -A celery_app worker
        --loglevel=info
        --concurrency=3
        --pool=prefork
        -Q celery
        --hostname=crawler-worker@celery-crawler-worker
    deploy:
      resources:
        limits:
          memory: 3G
        reservations:
          memory: 1.5G
    restart: unless-stopped

  celery-embedding:
    build:
      context: ./backend
      args:
        PYTHON_VERSION: ${PYTHON_VERSION:-3.11}
    container_name: rag-celery-embedding
    hostname: celery-embedding-worker
    volumes:
      - ./backend:/app:ro
    environment:
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - TZ=Asia/Seoul
    env_file:
      - .env
    depends_on:
      rabbitmq:
        condition: service_healthy
      redis:
        condition: service_healthy
      qdrant:
        condition: service_started
      ollama:
        condition: service_started
    command:
      - bash
      - -c
      - >
        source /opt/conda/bin/activate appenv &&
        celery -A celery_app worker
        --loglevel=info
        --concurrency=4
        --pool=prefork
        -Q embedding
        --hostname=embedding-worker@celery-embedding-worker
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    restart: unless-stopped

  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile.prod
      args:
        NEXT_PUBLIC_API_URL: ${NEXT_PUBLIC_API_URL}
        NEXT_PUBLIC_BACKEND_URL: ${NEXT_PUBLIC_BACKEND_URL:-http://api:8000}
        NEXT_PUBLIC_SUPABASE_URL: ${NEXT_PUBLIC_SUPABASE_URL}
        NEXT_PUBLIC_SUPABASE_KEY: ${NEXT_PUBLIC_SUPABASE_KEY}
        NEXTAUTH_URL: ${NEXTAUTH_URL}
        NEXTAUTH_SECRET: ${NEXTAUTH_SECRET}
        GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID}
        GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET}
        KAKAO_CLIENT_ID: ${KAKAO_CLIENT_ID}
        KAKAO_CLIENT_SECRET: ${KAKAO_CLIENT_SECRET}
        BACKEND_URL: ${BACKEND_URL}
    container_name: rag-frontend
    expose:
      - "3000"
    environment:
      - NEXT_PUBLIC_API_URL=${NEXT_PUBLIC_API_URL}
      - NEXT_PUBLIC_BACKEND_URL=${NEXT_PUBLIC_BACKEND_URL:-http://api:8000}
      - NEXT_PUBLIC_SUPABASE_URL=${NEXT_PUBLIC_SUPABASE_URL}
      - NEXT_PUBLIC_SUPABASE_KEY=${NEXT_PUBLIC_SUPABASE_KEY}
      - NEXTAUTH_URL=${NEXTAUTH_URL}
      - NEXTAUTH_SECRET=${NEXTAUTH_SECRET}
      - GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID}
      - GOOGLE_CLIENT_SECRET=${GOOGLE_CLIENT_SECRET}
      - KAKAO_CLIENT_ID=${KAKAO_CLIENT_ID}
      - KAKAO_CLIENT_SECRET=${KAKAO_CLIENT_SECRET}
      - BACKEND_URL=${BACKEND_URL}
    env_file:
      - .env
    depends_on:
      - api
    deploy:
      resources:
        limits:
          memory: 2G
        reservations:
          memory: 1G
    restart: unless-stopped

  rabbitmq:
    image: rabbitmq:3-management-alpine
    container_name: rag-rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER:-guest}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASS:-guest}
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "check_port_connectivity"]
      interval: 10s
      timeout: 5s
      retries: 10
      start_period: 40s
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: rag-redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    sysctls:
      - net.core.somaxconn=511
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  qdrant:
    image: qdrant/qdrant:latest
    container_name: rag-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      QDRANT__SERVICE__HTTP_PORT: 6333
      QDRANT__SERVICE__GRPC_PORT: 6334
      QDRANT__LOG_LEVEL: INFO
      QDRANT__STORAGE__WAL__WAL_CAPACITY_MB: 32
    deploy:
      resources:
        limits:
          memory: 1G
        reservations:
          memory: 512M
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    container_name: rag-ollama
    expose:
      - "11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    restart: unless-stopped

  ollama-init:
    image: curlimages/curl:latest
    container_name: rag-ollama-init
    depends_on:
      - ollama
    command:
      - /bin/sh
      - -c
      - |
        echo "=== Ollama 초기화 시작 ==="
        echo "Ollama 서비스 대기 중..."
        until curl -s http://ollama:11434/api/tags > /dev/null 2>&1; do
          echo "Ollama 대기 중... (5초 후 재시도)"
          sleep 5
        done
        echo "Ollama 서비스 연결 성공!"

        for MODEL in "qwen2.5:7b" "bge-m3"; do
          echo "모델 확인: $$MODEL"
          if curl -s http://ollama:11434/api/tags | grep -q "\"name\":\"$$MODEL\""; then
            echo "✓ 모델 $$MODEL 이미 존재"
          else
            echo "✗ 모델 $$MODEL 다운로드 시작..."
            curl -X POST http://ollama:11434/api/pull -d "{\"name\":\"$$MODEL\"}" || echo "경고: 모델 다운로드 실패"
            echo "✓ 모델 $$MODEL 처리 완료"
          fi
        done
        echo "=== Ollama 초기화 완료 ==="
    restart: "no"

volumes:
  rabbitmq_data:
  redis_data:
  qdrant_data:
  ollama_data: